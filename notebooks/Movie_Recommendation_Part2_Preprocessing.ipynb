{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "104b6b65",
   "metadata": {},
   "source": [
    "# Movie Recommendation System - PHASE 2: Data Preprocessing\n",
    "\n",
    "## Overview\n",
    "This notebook handles advanced data preprocessing including:\n",
    "- Advanced text processing and tokenization\n",
    "- Feature engineering and vectorization\n",
    "- Train-test splitting with stratification\n",
    "- Data quality enhancement\n",
    "- Standardization and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86ac96a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import processing libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data\n",
    "for resource in ['stopwords', 'wordnet']:\n",
    "    try:\n",
    "        nltk.data.find(f'corpora/{resource}')\n",
    "    except LookupError:\n",
    "        nltk.download(resource, quiet=True)\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e72e64",
   "metadata": {},
   "source": [
    "## Section 1: Load EDA Results and Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "981b9f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ EDA results loaded successfully!\n",
      "\n",
      "Loaded Data Shape: (4389, 28)\n",
      "Loaded Features Shape: (4389, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Load EDA results from Phase 1\n",
    "import os\n",
    "\n",
    "results_dir = '../results'\n",
    "\n",
    "# Load the pickled EDA results\n",
    "with open(os.path.join(results_dir, 'eda_results.pkl'), 'rb') as f:\n",
    "    eda_results = pickle.load(f)\n",
    "\n",
    "# Extract components\n",
    "df_movies = eda_results['movies_df'].copy()\n",
    "tfidf_vectorizer_desc = eda_results['tfidf_vectorizer']\n",
    "mlb_genres = eda_results['mlb_genres']\n",
    "scaler_numeric = eda_results['scaler']\n",
    "tfidf_matrix_desc = eda_results['tfidf_matrix_desc']\n",
    "genres_df = eda_results['genres_df']\n",
    "numeric_features_df = eda_results['numeric_features_df']\n",
    "\n",
    "print(\"✓ EDA results loaded successfully!\")\n",
    "print(f\"\\nLoaded Data Shape: {df_movies.shape}\")\n",
    "print(f\"Loaded Features Shape: {tfidf_matrix_desc.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043588ea",
   "metadata": {},
   "source": [
    "## Section 2: Advanced Text Processing with Multiple Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b678aeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying advanced text processing...\n",
      "✓ Text processing completed!\n",
      "\n",
      "Sample processed text:\n",
      "\n",
      "Movie: Avatar\n",
      "Original (first 100 chars): In the 22nd century, a paraplegic Marine is dispatched to the moon Pandora on a unique mission, but \n",
      "Processed (first 100 chars): 22nd centuri parapleg marin dispatch moon pandora uniqu mission becom torn follow order protect alie\n",
      "\n",
      "Movie: Pirates of the Caribbean: At World's End\n",
      "Original (first 100 chars): Captain Barbossa, long believed to be dead, has come back to life and is headed to the edge of the E\n",
      "Processed (first 100 chars): captain barbossa long believ dead come back life head edg earth turner elizabeth swann noth quit see\n",
      "✓ Text processing completed!\n",
      "\n",
      "Sample processed text:\n",
      "\n",
      "Movie: Avatar\n",
      "Original (first 100 chars): In the 22nd century, a paraplegic Marine is dispatched to the moon Pandora on a unique mission, but \n",
      "Processed (first 100 chars): 22nd centuri parapleg marin dispatch moon pandora uniqu mission becom torn follow order protect alie\n",
      "\n",
      "Movie: Pirates of the Caribbean: At World's End\n",
      "Original (first 100 chars): Captain Barbossa, long believed to be dead, has come back to life and is headed to the edge of the E\n",
      "Processed (first 100 chars): captain barbossa long believ dead come back life head edg earth turner elizabeth swann noth quit see\n"
     ]
    }
   ],
   "source": [
    "# Advanced text preprocessing with stemming and lemmatization\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def advanced_text_processing(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text processing:\n",
    "    - Lowercase conversion\n",
    "    - Punctuation removal\n",
    "    - Tokenization\n",
    "    - Stopword removal\n",
    "    - Stemming AND Lemmatization\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters, keep alphanumeric and spaces\n",
    "    import string\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenization (simple split instead of word_tokenize to avoid punkt_tab dependency)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    # Apply both stemming and lemmatization\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        stemmed = stemmer.stem(token)\n",
    "        lemmatized = lemmatizer.lemmatize(token)\n",
    "        # Use the shorter result (usually more conservative)\n",
    "        processed_tokens.append(min([stemmed, lemmatized], key=len))\n",
    "    \n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Apply advanced processing to overview text\n",
    "print(\"Applying advanced text processing...\")\n",
    "df_movies['overview_processed'] = df_movies['overview'].apply(advanced_text_processing)\n",
    "\n",
    "print(\"✓ Text processing completed!\")\n",
    "print(\"\\nSample processed text:\")\n",
    "for i in range(2):\n",
    "    print(f\"\\nMovie: {df_movies.iloc[i]['title']}\")\n",
    "    print(f\"Original (first 100 chars): {df_movies.iloc[i]['overview'][:100]}\")\n",
    "    print(f\"Processed (first 100 chars): {df_movies.iloc[i]['overview_processed'][:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12054314",
   "metadata": {},
   "source": [
    "## Section 3: Vectorization and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea55cea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating enhanced TF-IDF vectorization on processed text...\n",
      "✓ Processed TF-IDF Matrix Shape: (4389, 1500)\n",
      "\n",
      "Creating tags vectorization...\n",
      "✓ Processed TF-IDF Matrix Shape: (4389, 1500)\n",
      "\n",
      "Creating tags vectorization...\n",
      "✓ Tags TF-IDF Matrix Shape: (4389, 500)\n",
      "\n",
      "Combining all features...\n",
      "✓ Combined Feature Matrix Shape: (4389, 2023)\n",
      "  Total Features: 2023\n",
      "  Sparsity: 98.27%\n",
      "✓ Tags TF-IDF Matrix Shape: (4389, 500)\n",
      "\n",
      "Combining all features...\n",
      "✓ Combined Feature Matrix Shape: (4389, 2023)\n",
      "  Total Features: 2023\n",
      "  Sparsity: 98.27%\n"
     ]
    }
   ],
   "source": [
    "# Create enhanced TF-IDF on processed text\n",
    "print(\"Creating enhanced TF-IDF vectorization on processed text...\")\n",
    "tfidf_vectorizer_processed = TfidfVectorizer(\n",
    "    max_features=1500,           # Increased from 1000\n",
    "    ngram_range=(1, 3),          # Include trigrams\n",
    "    min_df=1,                    # Minimum document frequency\n",
    "    max_df=0.9,                  # Maximum document frequency\n",
    "    sublinear_tf=True            # Sublinear term frequency scaling\n",
    ")\n",
    "\n",
    "tfidf_matrix_processed = tfidf_vectorizer_processed.fit_transform(df_movies['overview_processed'])\n",
    "print(f\"✓ Processed TF-IDF Matrix Shape: {tfidf_matrix_processed.shape}\")\n",
    "\n",
    "# Create combined tags vectorization\n",
    "print(\"\\nCreating tags vectorization...\")\n",
    "tags_text = df_movies['tags_cleaned'].fillna('')\n",
    "tfidf_vectorizer_tags = TfidfVectorizer(\n",
    "    max_features=500,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=1,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "tfidf_matrix_tags = tfidf_vectorizer_tags.fit_transform(tags_text)\n",
    "print(f\"✓ Tags TF-IDF Matrix Shape: {tfidf_matrix_tags.shape}\")\n",
    "\n",
    "# Combine all features\n",
    "print(\"\\nCombining all features...\")\n",
    "combined_feature_matrix = hstack([\n",
    "    tfidf_matrix_processed * 0.4,    # Weight descriptions 40%\n",
    "    tfidf_matrix_tags * 0.3,         # Weight tags 30%\n",
    "    genres_df.values * 0.2,          # Weight genres 20%\n",
    "    numeric_features_df.values * 0.1 # Weight numeric features 10%\n",
    "])\n",
    "\n",
    "print(f\"✓ Combined Feature Matrix Shape: {combined_feature_matrix.shape}\")\n",
    "print(f\"  Total Features: {combined_feature_matrix.shape[1]}\")\n",
    "print(f\"  Sparsity: {100 * (1 - combined_feature_matrix.nnz / (combined_feature_matrix.shape[0] * combined_feature_matrix.shape[1])):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844c192b",
   "metadata": {},
   "source": [
    "## Section 4: Train-Test Split with Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "983fe188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating Distribution:\n",
      "rating_category\n",
      "Low            66\n",
      "Medium       1692\n",
      "High         2589\n",
      "Very High      42\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Performing stratified train-test split (80-20)...\n",
      "✓ Train indices: 3511\n",
      "✓ Test indices: 878\n",
      "\n",
      "Train set shape: (3511, 30)\n",
      "Test set shape: (878, 30)\n",
      "\n",
      "Train set rating distribution:\n",
      "rating_category\n",
      "Low            53\n",
      "Medium       1353\n",
      "High         2071\n",
      "Very High      34\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set rating distribution:\n",
      "rating_category\n",
      "Low           13\n",
      "Medium       339\n",
      "High         518\n",
      "Very High      8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ Train-test split completed!\n"
     ]
    }
   ],
   "source": [
    "# Create stratification column based on rating categories\n",
    "df_movies['rating_category'] = pd.cut(df_movies['vote_average'], \n",
    "                                        bins=[0, 4, 6, 8, 10],\n",
    "                                        labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "print(\"Rating Distribution:\")\n",
    "print(df_movies['rating_category'].value_counts().sort_index())\n",
    "\n",
    "# Perform stratified train-test split (80-20)\n",
    "print(\"\\nPerforming stratified train-test split (80-20)...\")\n",
    "\n",
    "indices = np.arange(len(df_movies))\n",
    "\n",
    "train_indices, test_indices = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.2,\n",
    "    stratify=df_movies['rating_category'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"✓ Train indices: {len(train_indices)}\")\n",
    "print(f\"✓ Test indices: {len(test_indices)}\")\n",
    "\n",
    "# Convert combined_feature_matrix to CSR format for efficient indexing\n",
    "from scipy.sparse import csr_matrix\n",
    "combined_feature_matrix = csr_matrix(combined_feature_matrix)\n",
    "\n",
    "# Split all features\n",
    "train_features = combined_feature_matrix[train_indices]\n",
    "test_features = combined_feature_matrix[test_indices]\n",
    "\n",
    "# Create training and testing dataframes\n",
    "train_df = df_movies.iloc[train_indices].reset_index(drop=True)\n",
    "test_df = df_movies.iloc[test_indices].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTrain set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "\n",
    "print(f\"\\nTrain set rating distribution:\")\n",
    "print(train_df['rating_category'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nTest set rating distribution:\")\n",
    "print(test_df['rating_category'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n✓ Train-test split completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fe4035",
   "metadata": {},
   "source": [
    "## Section 5: Create User-Item Interaction Matrix (for Collaborative Filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4113f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating synthetic user-item interaction matrix...\n",
      "✓ User-Item Matrix Shape: (150, 4389)\n",
      "✓ Sparsity: 74.10%\n",
      "✓ Non-zero interactions: 170524\n",
      "\n",
      "Calculating user similarity matrix...\n",
      "✓ User-User Similarity Shape: (150, 150)\n",
      "\n",
      "Calculating item similarity matrix...\n",
      "✓ User-Item Matrix Shape: (150, 4389)\n",
      "✓ Sparsity: 74.10%\n",
      "✓ Non-zero interactions: 170524\n",
      "\n",
      "Calculating user similarity matrix...\n",
      "✓ User-User Similarity Shape: (150, 150)\n",
      "\n",
      "Calculating item similarity matrix...\n",
      "✓ Item-Item Similarity Shape: (4389, 4389)\n",
      "✓ Item-Item Similarity Shape: (4389, 4389)\n"
     ]
    }
   ],
   "source": [
    "# Create synthetic user-item interaction matrix for collaborative filtering\n",
    "print(\"Creating synthetic user-item interaction matrix...\")\n",
    "\n",
    "n_users = 150\n",
    "n_movies = len(df_movies)\n",
    "\n",
    "# Create sparse user-item matrix\n",
    "np.random.seed(42)\n",
    "user_indices = []\n",
    "movie_indices = []\n",
    "ratings = []\n",
    "\n",
    "# Generate ratings for 30% of user-movie pairs\n",
    "sparsity = 0.7\n",
    "n_interactions = int(n_users * n_movies * (1 - sparsity))\n",
    "\n",
    "for _ in range(n_interactions):\n",
    "    user_id = np.random.randint(0, n_users)\n",
    "    movie_id = np.random.randint(0, n_movies)\n",
    "    # Generate rating based on movie's popularity and quality\n",
    "    base_rating = df_movies.iloc[movie_id]['vote_average']\n",
    "    rating = base_rating + np.random.normal(0, 1.5)\n",
    "    rating = np.clip(rating, 1, 10)\n",
    "    \n",
    "    user_indices.append(user_id)\n",
    "    movie_indices.append(movie_id)\n",
    "    ratings.append(rating)\n",
    "\n",
    "# Create sparse user-item matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "user_item_matrix = csr_matrix(\n",
    "    (ratings, (user_indices, movie_indices)),\n",
    "    shape=(n_users, n_movies)\n",
    ")\n",
    "\n",
    "print(f\"✓ User-Item Matrix Shape: {user_item_matrix.shape}\")\n",
    "print(f\"✓ Sparsity: {100 * (1 - user_item_matrix.nnz / (n_users * n_movies)):.2f}%\")\n",
    "print(f\"✓ Non-zero interactions: {user_item_matrix.nnz}\")\n",
    "\n",
    "# Calculate user-user and item-item similarity matrices for collaborative filtering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"\\nCalculating user similarity matrix...\")\n",
    "user_similarity = cosine_similarity(user_item_matrix)\n",
    "print(f\"✓ User-User Similarity Shape: {user_similarity.shape}\")\n",
    "\n",
    "print(\"\\nCalculating item similarity matrix...\")\n",
    "item_similarity = cosine_similarity(user_item_matrix.T)\n",
    "print(f\"✓ Item-Item Similarity Shape: {item_similarity.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac2cd86",
   "metadata": {},
   "source": [
    "## Section 6: Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6142e09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessed data saved to: ../results\\preprocessed_data.pkl\n",
      "✓ Train/Test sets saved to CSV\n",
      "\n",
      "================================================================================\n",
      "PREPROCESSING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Total Movies: 4389\n",
      "Training Movies: 3511 (80.0%)\n",
      "Testing Movies: 878 (20.0%)\n",
      "\n",
      "Feature Dimensions:\n",
      "  - Combined Features: 2023\n",
      "  - TF-IDF (Descriptions): 1500\n",
      "  - TF-IDF (Tags): 500\n",
      "  - Genres (One-Hot): 20\n",
      "  - Numeric Features: 3\n",
      "\n",
      "Collaborative Filtering Data:\n",
      "  - Users: 150\n",
      "  - Movies: 4389\n",
      "  - User-Item Interactions: 170524\n",
      "  - Sparsity: 74.10%\n",
      "\n",
      "✓ Preprocessing Phase completed successfully!\n",
      "✓ Train/Test sets saved to CSV\n",
      "\n",
      "================================================================================\n",
      "PREPROCESSING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Total Movies: 4389\n",
      "Training Movies: 3511 (80.0%)\n",
      "Testing Movies: 878 (20.0%)\n",
      "\n",
      "Feature Dimensions:\n",
      "  - Combined Features: 2023\n",
      "  - TF-IDF (Descriptions): 1500\n",
      "  - TF-IDF (Tags): 500\n",
      "  - Genres (One-Hot): 20\n",
      "  - Numeric Features: 3\n",
      "\n",
      "Collaborative Filtering Data:\n",
      "  - Users: 150\n",
      "  - Movies: 4389\n",
      "  - User-Item Interactions: 170524\n",
      "  - Sparsity: 74.10%\n",
      "\n",
      "✓ Preprocessing Phase completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save all preprocessed data\n",
    "import os\n",
    "\n",
    "results_dir = '../results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "preprocessing_results = {\n",
    "    'train_df': train_df,\n",
    "    'test_df': test_df,\n",
    "    'train_features': train_features,\n",
    "    'test_features': test_features,\n",
    "    'combined_feature_matrix': combined_feature_matrix,\n",
    "    'tfidf_vectorizer_processed': tfidf_vectorizer_processed,\n",
    "    'tfidf_vectorizer_tags': tfidf_vectorizer_tags,\n",
    "    'mlb_genres': mlb_genres,\n",
    "    'train_indices': train_indices,\n",
    "    'test_indices': test_indices,\n",
    "    'user_item_matrix': user_item_matrix,\n",
    "    'user_similarity': user_similarity,\n",
    "    'item_similarity': item_similarity\n",
    "}\n",
    "\n",
    "# Save to pickle\n",
    "pickle_path = os.path.join(results_dir, 'preprocessed_data.pkl')\n",
    "with open(pickle_path, 'wb') as f:\n",
    "    pickle.dump(preprocessing_results, f)\n",
    "\n",
    "print(f\"✓ Preprocessed data saved to: {pickle_path}\")\n",
    "\n",
    "# Also save train and test dataframes as CSV for reference\n",
    "train_df.to_csv(os.path.join(results_dir, 'train_movies.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(results_dir, 'test_movies.csv'), index=False)\n",
    "\n",
    "print(f\"✓ Train/Test sets saved to CSV\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal Movies: {len(df_movies)}\")\n",
    "print(f\"Training Movies: {len(train_df)} ({len(train_df)/len(df_movies)*100:.1f}%)\")\n",
    "print(f\"Testing Movies: {len(test_df)} ({len(test_df)/len(df_movies)*100:.1f}%)\")\n",
    "print(f\"\\nFeature Dimensions:\")\n",
    "print(f\"  - Combined Features: {combined_feature_matrix.shape[1]}\")\n",
    "print(f\"  - TF-IDF (Descriptions): {tfidf_matrix_processed.shape[1]}\")\n",
    "print(f\"  - TF-IDF (Tags): {tfidf_matrix_tags.shape[1]}\")\n",
    "print(f\"  - Genres (One-Hot): {genres_df.shape[1]}\")\n",
    "print(f\"  - Numeric Features: {numeric_features_df.shape[1]}\")\n",
    "print(f\"\\nCollaborative Filtering Data:\")\n",
    "print(f\"  - Users: {n_users}\")\n",
    "print(f\"  - Movies: {n_movies}\")\n",
    "print(f\"  - User-Item Interactions: {user_item_matrix.nnz}\")\n",
    "print(f\"  - Sparsity: {100 * (1 - user_item_matrix.nnz / (n_users * n_movies)):.2f}%\")\n",
    "print(f\"\\n✓ Preprocessing Phase completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
