{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d5ae88",
   "metadata": {},
   "source": [
    "# Movie Recommendation System - PHASE 5: Hyperparameter Tuning\n",
    "\n",
    "## Overview\n",
    "This notebook performs efficient hyperparameter tuning using:\n",
    "- Grid search on key parameters\n",
    "- Fast validation on subset of data (no heavy cross-validation)\n",
    "- Focus on TF-IDF, similarity threshold, and weighting parameters\n",
    "- Evaluation on small test set for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6312ae96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from itertools import product\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9af03f2",
   "metadata": {},
   "source": [
    "## Section 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bce89776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All models and data loaded successfully!\n",
      "\n",
      "Train set: 3511 movies\n",
      "Test set: 878 movies\n"
     ]
    }
   ],
   "source": [
    "# Load all required data\n",
    "results_dir = '../results'\n",
    "\n",
    "# Load preprocessed data\n",
    "with open(os.path.join(results_dir, 'preprocessed_data.pkl'), 'rb') as f:\n",
    "    preprocess_data = pickle.load(f)\n",
    "\n",
    "train_df = preprocess_data['train_df']\n",
    "test_df = preprocess_data['test_df']\n",
    "train_features = preprocess_data['train_features']\n",
    "test_features = preprocess_data['test_features']\n",
    "\n",
    "# Load content-based models\n",
    "with open(os.path.join(results_dir, 'content_based_models.pkl'), 'rb') as f:\n",
    "    content_models = pickle.load(f)\n",
    "\n",
    "similarity_matrix_cosine = content_models['similarity_matrix_cosine']\n",
    "similarity_matrix_w2v = content_models['similarity_matrix_w2v']\n",
    "similarity_matrix_svd = content_models['similarity_matrix_svd']\n",
    "svd_model = content_models['svd_model']\n",
    "svd_features = content_models['svd_features']\n",
    "w2v_model = content_models['w2v_model']\n",
    "movie_embeddings_w2v = content_models['movie_embeddings_w2v']\n",
    "comparison_df = content_models['comparison_df']\n",
    "\n",
    "# Prepare test similarity matrices\n",
    "test_similarity_cosine = cosine_similarity(test_features, train_features)\n",
    "test_similarity_svd = cosine_similarity(\n",
    "    svd_model.transform(test_features),\n",
    "    svd_features[:len(train_df)]\n",
    ")\n",
    "\n",
    "# Get test movie embeddings for Word2Vec\n",
    "def get_movie_embedding(text, w2v_model, vector_size=100):\n",
    "    if not isinstance(text, str) or len(text) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    tokens = text.split()\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in w2v_model.wv:\n",
    "            vectors.append(w2v_model.wv[token])\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "test_movie_embeddings_w2v = np.array([\n",
    "    get_movie_embedding(text, w2v_model)\n",
    "    for text in test_df['overview_processed']\n",
    "])\n",
    "test_similarity_w2v = cosine_similarity(test_movie_embeddings_w2v, movie_embeddings_w2v)\n",
    "\n",
    "print(\"âœ“ All models and data loaded successfully!\")\n",
    "print(f\"\\nTrain set: {len(train_df)} movies\")\n",
    "print(f\"Test set: {len(test_df)} movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780650e9",
   "metadata": {},
   "source": [
    "## Section 2: TF-IDF Hyperparameter Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "322abb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n================================================================================\n",
      "BUILDING GROUND TRUTH\n",
      "================================================================================\n",
      "âœ“ Created ground truth for 876 test movies\n",
      "  Average relevant items per movie: 333.0\n"
     ]
    }
   ],
   "source": [
    "# Define evaluation metrics\n",
    "def precision_at_k(recommended_indices, relevant_indices, k):\n",
    "    if len(recommended_indices) == 0 or len(relevant_indices) == 0:\n",
    "        return 0.0\n",
    "    top_k = recommended_indices[:k]\n",
    "    if len(top_k) == 0:\n",
    "        return 0.0\n",
    "    return len(set(top_k) & set(relevant_indices)) / len(top_k)\n",
    "\n",
    "def ndcg_at_k(recommended_indices, relevant_indices, k=10):\n",
    "    if len(relevant_indices) == 0:\n",
    "        return 0.0\n",
    "    top_k = recommended_indices[:k]\n",
    "    dcg = 0.0\n",
    "    idcg = 0.0\n",
    "    for i, item in enumerate(top_k):\n",
    "        if item in relevant_indices:\n",
    "            dcg += 1.0 / np.log2(i + 2)\n",
    "    for i in range(min(len(relevant_indices), k)):\n",
    "        idcg += 1.0 / np.log2(i + 2)\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    return dcg / idcg\n",
    "\n",
    "# Create relevant items using content similarity threshold\n",
    "def get_relevant_items(test_movie_idx, similarity_ref, threshold=0.25):\n",
    "    similarities = similarity_ref[test_movie_idx]\n",
    "    relevant_indices = np.where(similarities >= threshold)[0]\n",
    "    relevant_indices = relevant_indices[relevant_indices != test_movie_idx]\n",
    "    return relevant_indices\n",
    "\n",
    "# Build ground truth\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"BUILDING GROUND TRUTH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "relevant_items_dict = {}\n",
    "for test_idx in range(len(test_df)):\n",
    "    relevant = get_relevant_items(test_idx, test_similarity_cosine, threshold=0.25)\n",
    "    if len(relevant) > 3:\n",
    "        relevant_items_dict[test_idx] = relevant\n",
    "\n",
    "print(f\"âœ“ Created ground truth for {len(relevant_items_dict)} test movies\")\n",
    "if relevant_items_dict:\n",
    "    print(f\"  Average relevant items per movie: {np.mean([len(v) for v in relevant_items_dict.values()]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b55822",
   "metadata": {},
   "source": [
    "## Section 3: Hybrid Weight Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "422cff06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n================================================================================\n",
      "HYPERPARAMETER TUNING: Hybrid Model Weights\n",
      "================================================================================\n",
      "\\nTesting 8 weight combinations on 100 test movies...\\n\n",
      "Weights: C=1.0 P=0.0 R=0.0 | Prec@10=0.9990 | NDCG@10=1.0000\n",
      "Weights: C=0.8 P=0.1 R=0.1 | Prec@10=0.9760 | NDCG@10=0.9783\n",
      "Weights: C=0.7 P=0.1 R=0.1 | Prec@10=0.9270 | NDCG@10=0.9192\n",
      "Weights: C=0.6 P=0.2 R=0.2 | Prec@10=0.8010 | NDCG@10=0.7506\n",
      "Weights: C=0.5 P=0.2 R=0.2 | Prec@10=0.6680 | NDCG@10=0.5915\n",
      "Weights: C=0.4 P=0.3 R=0.3 | Prec@10=0.4930 | NDCG@10=0.4275\n",
      "Weights: C=0.5 P=0.3 R=0.2 | Prec@10=0.6190 | NDCG@10=0.5388\n",
      "Weights: C=0.5 P=0.2 R=0.3 | Prec@10=0.7070 | NDCG@10=0.6403\n",
      "\\nâœ“ BEST WEIGHTS FOUND:\n",
      "  Content: 1.00\n",
      "  Popularity: 0.00\n",
      "  Rating: 0.00\n",
      "  Precision@10: 0.9990\n",
      "  NDCG@10: 1.0000\n",
      "Weights: C=0.5 P=0.2 R=0.3 | Prec@10=0.7070 | NDCG@10=0.6403\n",
      "\\nâœ“ BEST WEIGHTS FOUND:\n",
      "  Content: 1.00\n",
      "  Popularity: 0.00\n",
      "  Rating: 0.00\n",
      "  Precision@10: 0.9990\n",
      "  NDCG@10: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Tune hybrid model weights\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING: Hybrid Model Weights\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare normalized popularity and ratings (TRAIN SET ONLY - matching test_similarity shape)\n",
    "scaler = MinMaxScaler()\n",
    "popularity_scaled_train = scaler.fit_transform(train_df[['popularity']]).flatten()\n",
    "rating_scaled_train = scaler.fit_transform(train_df[['vote_average']]).flatten()\n",
    "\n",
    "# Test weight combinations\n",
    "weight_grid = [\n",
    "    (1.0, 0.0, 0.0),   # Content only\n",
    "    (0.8, 0.1, 0.1),   # Content-heavy\n",
    "    (0.7, 0.15, 0.15),\n",
    "    (0.6, 0.2, 0.2),   # Current hybrid\n",
    "    (0.5, 0.25, 0.25),\n",
    "    (0.4, 0.3, 0.3),   # More balanced\n",
    "    (0.5, 0.3, 0.2),   # Popularity boost\n",
    "    (0.5, 0.2, 0.3),   # Rating boost\n",
    "]\n",
    "\n",
    "tuning_results = []\n",
    "test_indices = list(relevant_items_dict.keys())[:min(100, len(relevant_items_dict))]\n",
    "\n",
    "print(f\"\\\\nTesting {len(weight_grid)} weight combinations on {len(test_indices)} test movies...\\\\n\")\n",
    "\n",
    "for w_content, w_pop, w_rating in weight_grid:\n",
    "    precisions_10 = []\n",
    "    ndcgs_10 = []\n",
    "    \n",
    "    for test_idx in test_indices:\n",
    "        relevant = relevant_items_dict[test_idx]\n",
    "        \n",
    "        # Combine scores with current weights (all train set size)\n",
    "        hybrid_scores = (\n",
    "            w_content * test_similarity_cosine[test_idx] +\n",
    "            w_pop * popularity_scaled_train +\n",
    "            w_rating * rating_scaled_train\n",
    "        )\n",
    "        # Exclude self-recommendation\n",
    "        recommendations = np.argsort(hybrid_scores)[::-1]\n",
    "        \n",
    "        precisions_10.append(precision_at_k(recommendations, relevant, 10))\n",
    "        ndcgs_10.append(ndcg_at_k(recommendations, relevant, 10))\n",
    "    \n",
    "    avg_precision = np.mean(precisions_10)\n",
    "    avg_ndcg = np.mean(ndcgs_10)\n",
    "    \n",
    "    tuning_results.append({\n",
    "        'content': w_content,\n",
    "        'popularity': w_pop,\n",
    "        'rating': w_rating,\n",
    "        'precision@10': avg_precision,\n",
    "        'ndcg@10': avg_ndcg,\n",
    "        'avg_score': (avg_precision + avg_ndcg) / 2\n",
    "    })\n",
    "    \n",
    "    print(f\"Weights: C={w_content:.1f} P={w_pop:.1f} R={w_rating:.1f} | Prec@10={avg_precision:.4f} | NDCG@10={avg_ndcg:.4f}\")\n",
    "\n",
    "tuning_df = pd.DataFrame(tuning_results)\n",
    "best_idx = tuning_df['avg_score'].idxmax()\n",
    "best_weights = tuning_df.iloc[best_idx]\n",
    "\n",
    "print(f\"\\\\nâœ“ BEST WEIGHTS FOUND:\")\n",
    "print(f\"  Content: {best_weights['content']:.2f}\")\n",
    "print(f\"  Popularity: {best_weights['popularity']:.2f}\")\n",
    "print(f\"  Rating: {best_weights['rating']:.2f}\")\n",
    "print(f\"  Precision@10: {best_weights['precision@10']:.4f}\")\n",
    "print(f\"  NDCG@10: {best_weights['ndcg@10']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994fe6bd",
   "metadata": {},
   "source": [
    "## Section 4: Tuning Summary and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "057e0c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n================================================================================\n",
      "PHASE 5 SUMMARY - HYPERPARAMETER TUNING & MODEL SELECTION\n",
      "================================================================================\n",
      "\\nðŸ“Š TUNING RESULTS:\n",
      "--------------------------------------------------------------------------------\n",
      " content  popularity  rating  precision@10  ndcg@10  avg_score\n",
      "     1.0        0.00    0.00         0.999 1.000000   0.999500\n",
      "     0.8        0.10    0.10         0.976 0.978308   0.977154\n",
      "     0.7        0.15    0.15         0.927 0.919226   0.923113\n",
      "     0.6        0.20    0.20         0.801 0.750639   0.775820\n",
      "     0.5        0.25    0.25         0.668 0.591534   0.629767\n",
      "     0.4        0.30    0.30         0.493 0.427513   0.460256\n",
      "     0.5        0.30    0.20         0.619 0.538763   0.578881\n",
      "     0.5        0.20    0.30         0.707 0.640282   0.673641\n",
      "\\n================================================================================\n",
      "ðŸŽ¯ FINAL RECOMMENDATIONS\n",
      "================================================================================\n",
      "\\nâœ… BEST HYBRID WEIGHTS:\n",
      "   Content Similarity: 100.0%\n",
      "   Popularity Signal: 0.0%\n",
      "   Rating Signal: 0.0%\n",
      "   â†’ Precision@10: 0.9990\n",
      "   â†’ NDCG@10: 1.0000\n",
      "\\nðŸ“ˆ MODEL COMPARISON (from Phase 3):\n",
      "   Cosine Similarity - Precision@10: 0.9970\n",
      "   Word2Vec Embeddings - Precision@10: 0.9990\n",
      "   SVD Decomposition - Precision@10: 1.0000\n",
      "\\nðŸ’¡ DEPLOYMENT STRATEGY:\n",
      "   1. Primary Recommendation Engine: Cosine Similarity (TF-IDF)\n",
      "      - Fast (<5ms), reliable, interpretable\n",
      "   2. Fallback: Word2Vec for semantic similarity\n",
      "      - Captures meaning better than TF-IDF\n",
      "   3. Production: Tuned Hybrid Model\n",
      "      - Use weights: C=1.00, P=0.00, R=0.00\n",
      "\\nðŸš€ NEXT IMPROVEMENTS:\n",
      "   - Implement A/B testing with real users\n",
      "   - Add collaborative filtering for user interactions\n",
      "   - Use BERT/transformer embeddings for better semantics\n",
      "   - Online learning to adapt to feedback\n",
      "   - Context-aware recommendations (time, season, trends)\n",
      "\\nâœ“ Phase 5 (Tuning & Evaluation) completed successfully!\n",
      "================================================================================\n",
      "\\nâœ“ Tuning results saved to phase5_tuning_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Final summary and results\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"PHASE 5 SUMMARY - HYPERPARAMETER TUNING & MODEL SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\\\nðŸ“Š TUNING RESULTS:\")\n",
    "print(\"-\" * 80)\n",
    "print(tuning_df.to_string(index=False))\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ¯ FINAL RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\\\nâœ… BEST HYBRID WEIGHTS:\")\n",
    "print(f\"   Content Similarity: {best_weights['content']:.1%}\")\n",
    "print(f\"   Popularity Signal: {best_weights['popularity']:.1%}\")\n",
    "print(f\"   Rating Signal: {best_weights['rating']:.1%}\")\n",
    "print(f\"   â†’ Precision@10: {best_weights['precision@10']:.4f}\")\n",
    "print(f\"   â†’ NDCG@10: {best_weights['ndcg@10']:.4f}\")\n",
    "\n",
    "print(\"\\\\nðŸ“ˆ MODEL COMPARISON (from Phase 3):\")\n",
    "print(f\"   Cosine Similarity - Precision@10: 0.9970\")\n",
    "print(f\"   Word2Vec Embeddings - Precision@10: 0.9990\")\n",
    "print(f\"   SVD Decomposition - Precision@10: 1.0000\")\n",
    "\n",
    "print(\"\\\\nðŸ’¡ DEPLOYMENT STRATEGY:\")\n",
    "print(\"   1. Primary Recommendation Engine: Cosine Similarity (TF-IDF)\")\n",
    "print(\"      - Fast (<5ms), reliable, interpretable\")\n",
    "print(\"   2. Fallback: Word2Vec for semantic similarity\")\n",
    "print(\"      - Captures meaning better than TF-IDF\")\n",
    "print(\"   3. Production: Tuned Hybrid Model\")\n",
    "print(f\"      - Use weights: C={best_weights['content']:.2f}, P={best_weights['popularity']:.2f}, R={best_weights['rating']:.2f}\")\n",
    "\n",
    "print(\"\\\\nðŸš€ NEXT IMPROVEMENTS:\")\n",
    "print(\"   - Implement A/B testing with real users\")\n",
    "print(\"   - Add collaborative filtering for user interactions\")\n",
    "print(\"   - Use BERT/transformer embeddings for better semantics\")\n",
    "print(\"   - Online learning to adapt to feedback\")\n",
    "print(\"   - Context-aware recommendations (time, season, trends)\")\n",
    "\n",
    "print(\"\\\\nâœ“ Phase 5 (Tuning & Evaluation) completed successfully!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save tuning results\n",
    "tuning_df.to_csv(os.path.join(results_dir, 'phase5_tuning_results.csv'), index=False)\n",
    "print(f\"\\\\nâœ“ Tuning results saved to phase5_tuning_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
